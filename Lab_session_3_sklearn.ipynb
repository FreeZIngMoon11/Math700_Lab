{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25968395",
   "metadata": {},
   "source": [
    "# Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7f50cb",
   "metadata": {},
   "source": [
    "scikit-learn is an open source Python machine learning module. It has a plethora of machine learning models and provides functions that are often needed for a machine learning workflow. As you will see, it has a nice and intuitive interface. It makes creating complicated machine learning workflows very easy. \n",
    "\n",
    "The full documentation for scikit-learn is located at https://scikit-learn.org/stable/. It contains numerous examples and detailed descriptions for each function (and its associated parameters). This notebook will give you a brief introduction on how to implement the basic machine learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a7285547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f095e7",
   "metadata": {},
   "source": [
    "The scikit-learn library contains a few small datasets that we can play around with\n",
    "\n",
    "Descriptions of the datasets are included here: https://scikit-learn.org/stable/datasets/toy_dataset.html.\n",
    "        \n",
    "To illustrate a few concepts in data pre processing and ML pipeline, I will be using a toy dataset\n",
    "\n",
    "In later part of lab, you will predict closing stock price of a company by applying linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "819ef99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston #house price dataset\n",
    "\n",
    "boston_dataset = load_boston() #loading data\n",
    "boston_dataset.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9773bce9",
   "metadata": {},
   "source": [
    "**data** field in above dictionary represents the features of the data<br>\n",
    "**target** represents house price<br>\n",
    "**feature_names** correspond to names of features in data field<br>\n",
    "**DESCR** tells us about description of data<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c4b2ad67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>House Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   House Price\n",
       "0         24.0\n",
       "1         21.6\n",
       "2         34.7\n",
       "3         33.4\n",
       "4         36.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boston_df = pd.DataFrame(boston_dataset.data,columns = boston_dataset.feature_names)\n",
    "target_df = pd.DataFrame(boston_dataset.target, columns = ['House Price'])\n",
    "\n",
    "display(boston_df.head())\n",
    "display(target_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7547761",
   "metadata": {},
   "source": [
    "# Splitting Data:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d76928",
   "metadata": {},
   "source": [
    "Before interacting with the dataset any further, we must split the data into training and testing data. This way we can use the training data to explore the data and fit our models and then use the testing data to provide an unbiased measure of the performance of our models. \n",
    "\n",
    "This can be done using the train_test_split function in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8866b776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 13)\n",
      "(127, 13)\n",
      "(379, 1)\n",
      "(127, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data with a 75%-25% training-test split\n",
    "# set the random state for reproducible results else it can get give a different split everytime\n",
    "\n",
    "boston_X_train, boston_X_test, boston_y_train, boston_y_test = train_test_split(\n",
    "                                boston_df, target_df, test_size=0.25, random_state=500)\n",
    "\n",
    "#checking whether splitting has happened or not\n",
    "\n",
    "print(boston_X_train.shape)\n",
    "print(boston_X_test.shape)\n",
    "print(boston_y_train.shape)\n",
    "print(boston_y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b81b943",
   "metadata": {},
   "source": [
    "# Some Functionalities:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c63dfa",
   "metadata": {},
   "source": [
    "Scikit-learn relies heavily on object-oriented programming principles. It implements machine learning algorithms as classes and users create objects:-\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf8f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model_lr = LinearRegression() #declration of linear regression model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce41d8a",
   "metadata": {},
   "source": [
    "There are two main functionalities whenever we implement a ML model- Model fitting and prediction:-<br>\n",
    "\n",
    "**model_lr.fit(X, y)**: trains/fit the model to the feature matrix  ùëã  and its corresponding labels  ùë¶ <br>\n",
    "**model_lr.predict(X)**: makes predictions on the passed data set  ùëã ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093de9ee",
   "metadata": {},
   "source": [
    "Sometimes, we have to apply transformations on feature vectors as well:-<br>\n",
    "    \n",
    "**fit(X)**: trains/fits the transformation to the feature matrix  ùëã .<br>\n",
    "**transform(X)**: applies the transformation on  ùëã  using any parameters learned<br>\n",
    "**fit_transform(X)**: applies both fit(X) and then transform(X).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9e1c13",
   "metadata": {},
   "source": [
    "# Data Preprocessing:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db015c00",
   "metadata": {},
   "source": [
    "Often, the first step in data analysis is plotting the target variable or the value which we want to predict and see its distribution:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c6cb69bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2.,  1.,  5.,  6.,  2.,  3.,  6.,  6.,  5., 22., 11., 14., 12.,\n",
       "        11., 17., 22., 31., 23., 21., 20., 25., 13., 11.,  2.,  6.,  8.,\n",
       "         7.,  9.,  5.,  5.,  6.,  7.,  2.,  6.,  4.,  3.,  1.,  1.,  0.,\n",
       "         0.,  2.,  1.,  0.,  1.,  1.,  0.,  1.,  0.,  3.,  9.]),\n",
       " array([ 5. ,  5.9,  6.8,  7.7,  8.6,  9.5, 10.4, 11.3, 12.2, 13.1, 14. ,\n",
       "        14.9, 15.8, 16.7, 17.6, 18.5, 19.4, 20.3, 21.2, 22.1, 23. , 23.9,\n",
       "        24.8, 25.7, 26.6, 27.5, 28.4, 29.3, 30.2, 31.1, 32. , 32.9, 33.8,\n",
       "        34.7, 35.6, 36.5, 37.4, 38.3, 39.2, 40.1, 41. , 41.9, 42.8, 43.7,\n",
       "        44.6, 45.5, 46.4, 47.3, 48.2, 49.1, 50. ]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 842.4x595.44 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(boston_y_train, bins = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b837383",
   "metadata": {},
   "source": [
    "We can see from the above plot that the distribution looks like a normal distribution with some outliers:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc76cfb6",
   "metadata": {},
   "source": [
    "**1. Imputing Missing Data:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42df2e19",
   "metadata": {},
   "source": [
    "Missing data is a very common problem across all datasets. One simple strategy for addressing it is to impute missing values using a chosen strategy such as the \"mean\", \"median\" or some custom imputation. First, let's check for missing data in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "48fca6f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       0\n",
       "ZN         0\n",
       "INDUS      0\n",
       "CHAS       0\n",
       "NOX        0\n",
       "RM         0\n",
       "AGE        0\n",
       "DIS        0\n",
       "RAD        0\n",
       "TAX        0\n",
       "PTRATIO    0\n",
       "B          0\n",
       "LSTAT      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8a9195",
   "metadata": {},
   "source": [
    "As these are \"toy\" datasets, they do not have any missing values but we learnt missing value imputation in last lecture. Please refer to session 2 notebook for the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4eda6b",
   "metadata": {},
   "source": [
    "**2. Categorical Variable Handling(One Hot Encoding)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0801e91",
   "metadata": {},
   "source": [
    "Depending on the nature of our data and the ML models we want to implement, we will need to one-hot encode the categorical variables in our data. This means we will transform the feature into a set of dummy variables.\n",
    "\n",
    "I am suspecting **CHAS** and **RAD** might be categorical variables, lets check that:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f0ba6608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "[ 5.  4.  3. 24.  8.  1.  6.  7.  2.]\n"
     ]
    }
   ],
   "source": [
    "print(boston_X_train['CHAS'].unique())\n",
    "print(boston_X_train['RAD'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc21fd",
   "metadata": {},
   "source": [
    "RAD is a categorical variable which we will have to take care of in this case. \n",
    "\n",
    "The reason for the same is that any regression model will consider higher values of RAD to have greater effect on house prices. RAD just represents the index of accesibility, so we don't want any ordinality in that:- \n",
    "\n",
    "We will see how to apply one hot encoding only on this particular column:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "663593e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_X_train = pd.get_dummies(boston_X_train,prefix=['RAD'], columns = ['RAD'], drop_first=True)\n",
    "boston_X_test = pd.get_dummies(boston_X_test,prefix=['RAD'], columns = ['RAD'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521d4be7",
   "metadata": {},
   "source": [
    "We pass the drop_first argument in the above function as **True**, because if a categorical variable has n unique values, we only need to create (n-1) seperate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "adbd0ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>RAD_2.0</th>\n",
       "      <th>RAD_3.0</th>\n",
       "      <th>RAD_4.0</th>\n",
       "      <th>RAD_5.0</th>\n",
       "      <th>RAD_6.0</th>\n",
       "      <th>RAD_7.0</th>\n",
       "      <th>RAD_8.0</th>\n",
       "      <th>RAD_24.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1.42502</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.871</td>\n",
       "      <td>6.510</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.7659</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>364.31</td>\n",
       "      <td>7.39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.78420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.990</td>\n",
       "      <td>81.7</td>\n",
       "      <td>4.2579</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.75</td>\n",
       "      <td>14.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>0.09065</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.464</td>\n",
       "      <td>5.920</td>\n",
       "      <td>61.5</td>\n",
       "      <td>3.9175</td>\n",
       "      <td>223.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>391.34</td>\n",
       "      <td>13.65</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>5.09017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.713</td>\n",
       "      <td>6.297</td>\n",
       "      <td>91.8</td>\n",
       "      <td>2.3682</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>385.09</td>\n",
       "      <td>17.27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.13262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.520</td>\n",
       "      <td>5.851</td>\n",
       "      <td>96.7</td>\n",
       "      <td>2.1069</td>\n",
       "      <td>384.0</td>\n",
       "      <td>20.9</td>\n",
       "      <td>394.05</td>\n",
       "      <td>16.47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS    TAX  PTRATIO  \\\n",
       "159  1.42502   0.0  19.58   0.0  0.871  6.510  100.0  1.7659  403.0     14.7   \n",
       "17   0.78420   0.0   8.14   0.0  0.538  5.990   81.7  4.2579  307.0     21.0   \n",
       "269  0.09065  20.0   6.96   1.0  0.464  5.920   61.5  3.9175  223.0     18.6   \n",
       "452  5.09017   0.0  18.10   0.0  0.713  6.297   91.8  2.3682  666.0     20.2   \n",
       "105  0.13262   0.0   8.56   0.0  0.520  5.851   96.7  2.1069  384.0     20.9   \n",
       "\n",
       "          B  LSTAT  RAD_2.0  RAD_3.0  RAD_4.0  RAD_5.0  RAD_6.0  RAD_7.0  \\\n",
       "159  364.31   7.39        0        0        0        1        0        0   \n",
       "17   386.75  14.67        0        0        1        0        0        0   \n",
       "269  391.34  13.65        0        1        0        0        0        0   \n",
       "452  385.09  17.27        0        0        0        0        0        0   \n",
       "105  394.05  16.47        0        0        0        1        0        0   \n",
       "\n",
       "     RAD_8.0  RAD_24.0  \n",
       "159        0         0  \n",
       "17         0         0  \n",
       "269        0         0  \n",
       "452        0         1  \n",
       "105        0         0  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9084e9b3",
   "metadata": {},
   "source": [
    "**3. Feature Scaling:-**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b29bb4f",
   "metadata": {},
   "source": [
    "A lot of machine learning models work best when the features have similar scales.  This supports faster model convergence and removes any bias toward features\n",
    "\n",
    "Two of the most common techniques for feature normalization are:-\n",
    "\n",
    "1) StandardScaler<br>\n",
    "2) MinMaxScaler<br>\n",
    "\n",
    "StandardScaler works by adjusting the mean of each feature to zero with a standard deviation of 1<br>\n",
    "MinMaxScaler works by scaling all values to between 0 and 1<br>\n",
    "\n",
    "There are no hard rules for when to use one over the other but some factors to consider are the problem we intend to solve, assumptions regarding the distribution of the data (including the presence of outliers) and the ML models we plan on implementing. It can also be a good option to try out both and see which results in better performance.\n",
    "\n",
    "In this problem, we saw that target variable distribution is close to gaussian, so we will go ahead with StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fb7d7a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#fit_transform on training data\n",
    "boston_X_train_scal = scaler.fit_transform(boston_X_train)\n",
    "\n",
    "#transform on testing data\n",
    "boston_X_test_scal = scaler.transform(boston_X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d89f80a",
   "metadata": {},
   "source": [
    "**4. Feature Selection** :-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c7cdf2",
   "metadata": {},
   "source": [
    "Generally when we have large datasets with high dimensionality, feature selection can help us reduce the dimensionality by removing irrelevant or redundant features. <br>\n",
    "This can help with reducing the computational costs associated with training models and can also improve the performance of our models in some cases. \n",
    "\n",
    "There are many feature selection techniques, but for simplicity, we will be experimenting with mutual information and PCA here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c49863ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape (before applying feature selection): (379, 20)\n",
      "Testing shape (before applying feature selection): (127, 20)\n",
      "Training data shape (after applying Mutual Info): (379, 10)\n",
      "Testing data shape (after applying Mutual Info): (127, 10)\n",
      "Training shape (after applying PCA): (379, 12)\n",
      "Testing shape (after applying PCA): (127, 12)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "print(\"Training shape (before applying feature selection):\", boston_X_train_scal.shape)\n",
    "print(\"Testing shape (before applying feature selection):\", boston_X_test_scal.shape)\n",
    "\n",
    "# select the top 10 features\n",
    "    \n",
    "selector = SelectKBest(mutual_info_regression, k = 10)\n",
    "boston_X_train_mi = selector.fit_transform(boston_X_train_scal, boston_y_train)\n",
    "boston_X_test_mi = selector.transform(boston_X_test_scal)\n",
    "\n",
    "print(\"Training data shape (after applying Mutual Info):\", boston_X_train_mi.shape)\n",
    "print(\"Testing data shape (after applying Mutual Info):\", boston_X_test_mi.shape)\n",
    "\n",
    "#we can specify either the number of features we want after PCA or the percentage variance explained\n",
    "\n",
    "pca = PCA(n_components = 0.90)\n",
    "\n",
    "boston_X_train_pca = pca.fit_transform(boston_X_train_scal, boston_y_train)\n",
    "boston_X_test_pca = pca.transform(boston_X_test_scal)\n",
    "\n",
    "#checking dimensionality\n",
    "\n",
    "print(\"Training shape (after applying PCA):\", boston_X_train_pca.shape)\n",
    "print(\"Testing shape (after applying PCA):\", boston_X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35502b",
   "metadata": {},
   "source": [
    "# Model Fitting, Cross Validation and Evaluation:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96194fe4",
   "metadata": {},
   "source": [
    "Now we can finally fit some models. Scikit-learn allows you to easily implement a variety of models <br>\n",
    "\n",
    "Let's start by fitting two models - <br>\n",
    "\n",
    "1) Linear Regression <br>\n",
    "2) K-Nearest Neighbors Regression \n",
    "\n",
    "We will fit the models to each of the three feature sets - \n",
    "\n",
    "1) all of the features(no feature selection) <br>\n",
    "2) the feature subset selected using mutual information <br>\n",
    "3) the feature subset selected using PCA \n",
    "\n",
    "This will help us to compare performance both between the models and the different feature subsets. <br>\n",
    "\n",
    "We will use cross-validation (3 folds in this example) to get a more reliable estimate of the performance of our models without having to touch our test dataset yet. The default performance metric will be  $ùëÖ^{2}$, which is the standard metric used in regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "02f399ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:\n",
      "                      R^2 on Fold 1  R^2 on Fold 2  R^2 on Fold 3  Mean R^2\n",
      "No Feature Selection       0.774797       0.793699       0.539814  0.702770\n",
      "MI Features                0.737847       0.775111       0.582031  0.698330\n",
      "PCA Features               0.778266       0.777900       0.504378  0.686848\n",
      "\n",
      "K-Nearest Neighbors Regression:\n",
      "                      R^2 on Fold 1  R^2 on Fold 2  R^2 on Fold 3  Mean R^2\n",
      "No Feature Selection       0.681380       0.704307       0.523487  0.636391\n",
      "MI Features                0.798090       0.761181       0.763463  0.774244\n",
      "PCA Features               0.685634       0.703350       0.566385  0.651790\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr_scores_df = pd.DataFrame(columns=[\"R^2 on Fold 1\", \"R^2 on Fold 2\", \"R^2 on Fold 3\", \"Mean R^2\"])\n",
    "print(\"Linear Regression:\")\n",
    "lr_scal_scores = list(cross_val_score(lr, boston_X_train_scal, boston_y_train, cv=3))\n",
    "lr_scal_scores.append(np.mean(lr_scal_scores))\n",
    "lr_scores_df.loc[\"No Feature Selection\"] = lr_scal_scores\n",
    "lr_mi_scores = list(cross_val_score(lr, boston_X_train_mi, boston_y_train, cv=3))\n",
    "lr_mi_scores.append(np.mean(lr_mi_scores))\n",
    "lr_scores_df.loc[\"MI Features\"] = lr_mi_scores\n",
    "lr_pca_scores = list(cross_val_score(lr, boston_X_train_pca, boston_y_train, cv=3))\n",
    "lr_pca_scores.append(np.mean(lr_pca_scores))\n",
    "lr_scores_df.loc[\"PCA Features\"] = lr_pca_scores\n",
    "print(lr_scores_df.head())\n",
    "\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "knn_scores_df = pd.DataFrame(columns=[\"R^2 on Fold 1\", \"R^2 on Fold 2\", \"R^2 on Fold 3\", \"Mean R^2\"])\n",
    "print(\"\\nK-Nearest Neighbors Regression:\")\n",
    "knn_scal_scores = list(cross_val_score(knn, boston_X_train_scal, boston_y_train, cv=3))\n",
    "knn_scal_scores.append(np.mean(knn_scal_scores))\n",
    "knn_scores_df.loc[\"No Feature Selection\"] = knn_scal_scores\n",
    "knn_mi_scores = list(cross_val_score(knn, boston_X_train_mi, boston_y_train, cv=3))\n",
    "knn_mi_scores.append(np.mean(knn_mi_scores))\n",
    "knn_scores_df.loc[\"MI Features\"] = knn_mi_scores\n",
    "knn_pca_scores = list(cross_val_score(knn, boston_X_train_pca, boston_y_train, cv=3))\n",
    "knn_pca_scores.append(np.mean(knn_pca_scores))\n",
    "knn_scores_df.loc[\"PCA Features\"] = knn_pca_scores\n",
    "print(knn_scores_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d4edc6",
   "metadata": {},
   "source": [
    "Looking at the results, we see that linear regression performs similarily across the feature subsets but for KNN, but the model performs significantly better on the feature subset chosen using mutual information<br>\n",
    "\n",
    "We can also adjust the values of n_neighbor(number of neighbors in knn) as that is a hyperparameter. We can do a grid search on some of the values and pick the one which gives best score. This is called **Hyperparameter tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c3e9d435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'n_neighbors': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "# search over n_neighbors\n",
    "param_grid = [{'n_neighbors': [1, 2, 3, 4, 5, 6, 7] }]\n",
    "\n",
    "cv_knn = GridSearchCV(knn, param_grid, cv=2)\n",
    "cv_knn.fit(boston_X_train_mi, boston_y_train)\n",
    "print(\"Best Params:\", cv_knn.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aba9c56",
   "metadata": {},
   "source": [
    "# Predictions in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a8e426ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R^2: 0.9089643817091673\n",
      "Testing R^2: 0.8311400097923058\n",
      "MSE: 18.00459317585302\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=3)\n",
    "knn.fit(boston_X_train_mi, boston_y_train)\n",
    "predictions = knn.predict(boston_X_test_mi)\n",
    "\n",
    "print(\"Training R^2:\", knn.score(boston_X_train_mi, boston_y_train))\n",
    "print(\"Testing R^2:\", knn.score(boston_X_test_mi, boston_y_test))\n",
    "print(\"MSE:\", mean_squared_error(boston_y_test, predictions)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4d29b",
   "metadata": {},
   "source": [
    "**We can see that $R^{2}$ on testing data is pretty decent. So this completes our ML pipeline for this task**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f14894",
   "metadata": {},
   "source": [
    "# Exercise:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06c6e9a",
   "metadata": {},
   "source": [
    "Download the apple stock data of last one year from yahoo finance:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3164359b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad83bb26",
   "metadata": {},
   "source": [
    "Drop null values if any, and then split the data intro train and test with 80-20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f155a976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dca4403f",
   "metadata": {},
   "source": [
    "Fit a linear regression model to predict closing stock price from opening stock price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2fa9a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "587e9525",
   "metadata": {},
   "source": [
    "Plot a scatter plot of predictions vs test data values and report $R^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39052f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2488b167",
   "metadata": {},
   "source": [
    "**Read up about some of the functionalities of sklearn on your own, its very vast and plenty of good resources are avaialble online**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
