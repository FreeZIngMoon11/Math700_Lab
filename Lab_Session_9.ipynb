{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3429ed86",
   "metadata": {},
   "source": [
    "## Adaptive Learning Rate and Optimization:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30bf293",
   "metadata": {},
   "source": [
    "There are a vast variety of optimizers that we can use to optimize our network parameters. We will focus on the following: SGD with Momentum, AdaGrad, RMSProp and Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957a55a7",
   "metadata": {},
   "source": [
    "#### SGD with Momentum:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49a821f",
   "metadata": {},
   "source": [
    "SGD with momentum, in contrast to simple SGD, is a method that prevents oscillations during an update by adding the scaled previous update in the optimization to the current update:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{t+1} =& \\gamma v_{t} + \\alpha \\nabla_{\\theta_t} J(\\theta_t), \\nonumber \\\\\n",
    "\\theta_{t+1} =& \\theta_t + v_{t+1}, \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $v_0$ is initialized to zeros in the first optimization of the timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fd16bf",
   "metadata": {},
   "source": [
    "#### AdaGrad:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0085ed",
   "metadata": {},
   "source": [
    "Adagrad or adaptive gradient algorithm is a parameter update method that adapts the learning rate based on whether the parameters represent frequently ocurring features or infrecuently ocurring features.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g_{t} =& \\nabla_{\\theta_{t}} J(\\theta_{t}), \\nonumber \\\\\n",
    "\\theta_{t+1} =& \\theta_{t} - \\frac{\\alpha}{\\sqrt{G_{t} + \\epsilon}} \\odot g_{t}, \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $G_{t} \\in \\mathbb{R}^{d \\times d}$ is a diagonal matrix where $G_{i,i}$ is $\\sum^{t}_1 g_{t,i}^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cfe7b6",
   "metadata": {},
   "source": [
    "#### RMSProp:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e67dd2",
   "metadata": {},
   "source": [
    "RMSProp is similar to adagrad but with one difference: Instead of accumulating all past squared gradients, it looks at a window of previous gradients defined by a decaying factor.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}[g^2]_{t} =& \\gamma \\mathbb{E}[g^2]_{t-1} + (1 - \\gamma)g_{t}^2, \\nonumber \\\\\n",
    "\\theta_{t+1} =& \\theta_{t} - \\frac{\\alpha}{\\sqrt{\\mathbb{E}[g^2]_{t} + \\epsilon}} \\odot g_{t}, \\nonumber\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e158542d",
   "metadata": {},
   "source": [
    "### Adam:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb8fb24",
   "metadata": {},
   "source": [
    "Adam or adaptive moment estimation is another adaptive learning rate method with respect to each parameter. This method also keeps an exponentially decaying average of past gradients. However, in constrast to previous methods, a momentum like term is used which keeps an exponentially decaying average of past gradients. The parameter update is performed as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "m_{t} =& \\beta_1 m_{t-1} + (1 - \\beta_1)g_{t}, \\nonumber \\\\\n",
    "v_{t} =& \\beta_2 v_{t-1} + (1 - \\beta_2)g_t^2, \\nonumber \\\\\n",
    "\\hat{m}_t =& \\frac{m_t}{1-\\beta_1^t}, \\nonumber \\\\\n",
    "\\hat{v}_t =& \\frac{v_t}{1-\\beta_2^t}, \\nonumber \\\\\n",
    "\\theta_{t+1} =& \\theta_{t} - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon}\\hat{m}_t, \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $m_t$ and $v_t$ are the first and second moment estimates of the gradients while $\\hat{m}_t$ and $\\hat{v}_t$ are bias corrected values of the two moments. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5644116",
   "metadata": {},
   "source": [
    "## Batch Normalization:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5f3361",
   "metadata": {},
   "source": [
    "The description of batch normalization is motivated by Stanford CS231 course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff3870c",
   "metadata": {},
   "source": [
    "One way to make deep networks easier to train is to use more sophisticated optimization procedures such as SGD+momentum, RMSProp, or Adam. Another strategy is to change the architecture of the network to make it easier to train. One idea along these lines is batch normalization.\n",
    "\n",
    "The idea is relatively straightforward. Machine learning methods tend to work better when their input data consists of uncorrelated features with zero mean and unit variance. When training a neural network, we can preprocess the data before feeding it to the network to explicitly decorrelate its features; this will ensure that the first layer of the network sees data that follows a nice distribution. However even if we preprocess the input data, the activations at deeper layers of the network will likely no longer be decorrelated and will no longer have zero mean or unit variance since they are output from earlier layers in the network. Even worse, during the training process the distribution of features at each layer of the network will shift as the weights of each layer are updated.\n",
    "\n",
    " At training time, a batch normalization layer uses a minibatch of data to estimate the mean and standard deviation of each feature. These estimated means and standard deviations are then used to center and normalize the features of the minibatch. A running average of these means and standard deviations is kept during training, and at test time these running averages are used to center and normalize features.It is possible that this normalization strategy could reduce the representational power of the network, since it may sometimes be optimal for certain layers to have features that are not zero-mean or unit variance. To this end, the batch normalization layer includes learnable shift and scale parameters for each feature dimension."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
