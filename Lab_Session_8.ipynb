{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%logstop\n",
    "%logstart -rtq ~/.logs/ML_Natural_Language_Processing.py append\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "Natural language processing (NLP) is the field devoted to methods and algorithms for processing human (natural) languages for computers. NLP is a vast discipline that is actively being researched. For this notebook, we will be concerned with NLP tools and techniques we can use for machine learning applications. Some examples of machine learning applications using NLP include sentiment analysis, topic modeling, and language translation. In NLP, the following terms have specific meanings:\n",
    "\n",
    "* **Corpus**: The body/collection of text being investigated.\n",
    "* **Document**: The unit of analysis, what is considered a single observation.\n",
    "\n",
    "Examples of corpora include a collection of reviews and tweets, the text of the _Iliad_, and Wikipedia articles. Documents can be whatever you decided, it is what your model will consider an observation. For the example when the corpus is a collection of reviews or tweets, it is logical to make the document a single review or tweet. For the example of the text of the _Iliad_, we can set the document size to a sentence or a paragraph. The choice of document size will be influenced by the size of our corpus. If it is large, it may make sense to call each paragraph a document. As is usually the case, some design choices that need to be made.\n",
    "\n",
    "For this notebook, we will build a classifier to discern homonyms, words that are spelled the same but that have different meanings. The exact use case we will explore is to discern if the word \"python\" refers to the programming language or the animal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP with spaCy\n",
    "\n",
    "spaCy is a Python package that bills itself as \"industrial-strength\" natural language processing. We will use the tools spaCy provides in conjunction with scikit-learn. More about spaCy can be found [here](https://spacy.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's try out spacy.\n",
      "We can easily divide our text into sentences!\n",
      "I've run out of ideas.\n",
      "Let\n",
      "We\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# load text processing pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# nlp accepts a string\n",
    "doc = nlp(\"Let's try out spacy. We can easily divide our text into sentences! I've run out of ideas.\")\n",
    "\n",
    "# iterate through each sentence\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "\n",
    "# index words\n",
    "print(doc[0])\n",
    "print(doc[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another nice feature from spaCy is part-of-speech tagging, the process of identifying whether a word is a noun, adjective, adverb, etc. A processed word has the attribute `pos_` and `tag_`; the former identifies the simple part of speech (e.g., noun) wile the latter identifies the more detailed part of speech (e.g., proper noun). The meaning of the resulting abbreviations of the `tag_` are listed [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) or can be revealed by running `spacy.explain` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'DET', 'DT')\n",
      "('quick', 'ADJ', 'JJ')\n",
      "('brown', 'ADJ', 'JJ')\n",
      "('fox', 'NOUN', 'NN')\n",
      "('jumped', 'VERB', 'VBD')\n",
      "('over', 'ADP', 'IN')\n",
      "('the', 'DET', 'DT')\n",
      "('lazy', 'ADJ', 'JJ')\n",
      "('dog', 'NOUN', 'NN')\n",
      "('.', 'PUNCT', '.')\n",
      "('Mr.', 'PROPN', 'NNP')\n",
      "('Peanut', 'PROPN', 'NNP')\n",
      "('wears', 'VERB', 'VBZ')\n",
      "('a', 'DET', 'DT')\n",
      "('top', 'ADJ', 'JJ')\n",
      "('hat', 'NOUN', 'NN')\n",
      "('.', 'PUNCT', '.')\n",
      "\n",
      "VBZ verb, 3rd person singular present\n",
      "DT determiner\n",
      "NN noun, singular or mass\n",
      "IN conjunction, subordinating or preposition\n",
      "VBD verb, past tense\n",
      "NNP noun, proper singular\n",
      ". punctuation mark, sentence closer\n",
      "JJ adjective (English), other noun-modifier (Chinese)\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"The quick brown fox jumped over the lazy dog. Mr. Peanut wears a top hat.\")\n",
    "tags = set()\n",
    "\n",
    "# reveal part of speech\n",
    "for word in doc:\n",
    "    tags.add(word.tag_)\n",
    "    print((word.text, word.pos_, word.tag_))\n",
    "\n",
    "# revealing meaning of tags\n",
    "print()\n",
    "for tag in tags:\n",
    "    print(tag, spacy.explain(tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining a corpus\n",
    "\n",
    "Before we can move on with our analysis, we need to obtain a corpus. For our intended classifier, we need documents pertaining to python the animal and Python the programming language. Let's use Wikipedia articles to form our corpus. Luckily, there's a Python package called `wikipedia` that makes it easy to fetch articles. We will create documents based on the sentences in the articles. The function allows us to pass multiples pages in constructing the documents, allowing us to prevent one class of documents from dominating the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python is an interpreted high-level general-purpose programming language.', 'Its design philosophy emphasizes code readability with its use of significant indentation.', 'Its language constructs as well as its object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.', 'Python is dynamically-typed and garbage-collected.', 'It supports multiple programming paradigms, including structured (particularly, procedural), object-oriented and functional programming.']\n",
      "\n",
      "['The reticulated python (Malayopython reticulatus) is a python species native to South and Southeast Asia.', \"It is the world's longest snake, and listed as least concern on the IUCN Red List because of its wide distribution.\", 'In several countries in its range, it is hunted for its skin, for use in traditional medicine, and for sale as a pet.', 'It is an excellent swimmer, has been reported far out at sea, and has colonized many small islands within its range.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "\n",
    "def pages_to_sentences(*pages):\n",
    "    \"\"\"Return a list of sentences in Wikipedia articles.\"\"\"\n",
    "    sentences = []\n",
    "    \n",
    "    for page in pages:\n",
    "        p = wikipedia.page(page)\n",
    "        doc = nlp(p.content)\n",
    "        sentences += [sent.text for sent in doc.sents]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "animal_sents = pages_to_sentences(\"Reticulated python\", \"Ball Python\")\n",
    "language_sents = pages_to_sentences(\"Python (programming language)\")\n",
    "documents = animal_sents + language_sents\n",
    "\n",
    "print(language_sents[:5])\n",
    "print()\n",
    "print(animal_sents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words model\n",
    "\n",
    "Machine learning models needs to ingest data in a structured form, a matrix where the rows represents observations and the columns are features/attributes. When working with text data, we need a method to convert this unstructured data into a form that the machine learning model can work with. Let's consider our motivating example to create a classifier to discern the usage of \"python\" in a document. We understand that documents referring to the programming language will use words such as \"integer\", \"byte\", and \"error\" at higher frequency than documents that refer to python the animal. The reverse is true for words such as \"bite\", \"snake\", and \"pet\". One technique to _transform_ text data into a matrix is to count the number of appearances of each word in each document. This technique is called the **bag of words** model. The model gets its name because each document is viewed as a bag holding all the words, disregarding word order, context, and grammar. After applying the bag of words model to a corpus, the resulting matrix will exhibit patterns that a machine learning model can exploit. See the example below for the result of applying the bag of words model to a corpus of two documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document 0: \"The python is a large snake, although the snake is not venomous.\" <br>\n",
    "Document 1: \"Python is an interpreted programming language for general purpose programming.\" <br>\n",
    "<br>\n",
    "\n",
    "| although | an | for | general | interpreted | is | language | large | not | programming | purpose | python | snake | the | venomous |\n",
    "|:--------:|----|-----|---------|-------------|----|----------|-------|-----|-------------|---------|--------|-------|-----|----------|\n",
    "|     1    | 0  | 0   | 0       | 0           | 2  | 0        | 1     | 1   | 0           | 0       | 1      | 2     | 2   | 1        |\n",
    "|     0    | 1  | 1   | 1       | 1           | 1  | 1        | 0     | 0   | 2           | 1       | 1      | 0     | 0   | 0        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer :-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 230)\t1\n",
      "  (0, 282)\t1\n",
      "  (0, 1363)\t1\n",
      "  (0, 1547)\t1\n",
      "  (0, 1705)\t1\n",
      "  (0, 2034)\t2\n",
      "  (0, 2161)\t1\n",
      "  (0, 2163)\t1\n",
      "  (0, 2383)\t1\n",
      "  (0, 2384)\t1\n",
      "  (0, 2396)\t1\n",
      "  (0, 2564)\t1\n",
      "  (0, 2596)\t1\n",
      "  (1, 230)\t1\n",
      "  (1, 281)\t1\n",
      "  (1, 346)\t1\n",
      "  (1, 602)\t1\n",
      "  (1, 794)\t1\n",
      "  (1, 1363)\t1\n",
      "  (1, 1368)\t1\n",
      "  (1, 1380)\t1\n",
      "  (1, 1382)\t1\n",
      "  (1, 1462)\t1\n",
      "  (1, 1496)\t1\n",
      "  (1, 1497)\t1\n",
      "  :\t:\n",
      "  (853, 2034)\t1\n",
      "  (854, 258)\t1\n",
      "  (855, 68)\t1\n",
      "  (855, 96)\t1\n",
      "  (855, 138)\t1\n",
      "  (855, 1364)\t1\n",
      "  (857, 52)\t1\n",
      "  (857, 1573)\t1\n",
      "  (857, 2495)\t1\n",
      "  (858, 75)\t1\n",
      "  (858, 846)\t1\n",
      "  (858, 1268)\t1\n",
      "  (858, 1990)\t1\n",
      "  (858, 2034)\t1\n",
      "  (859, 169)\t1\n",
      "  (859, 1984)\t1\n",
      "  (859, 2757)\t1\n",
      "  (860, 81)\t1\n",
      "  (860, 123)\t1\n",
      "  (860, 138)\t1\n",
      "  (860, 1364)\t1\n",
      "  (863, 975)\t1\n",
      "  (863, 1493)\t1\n",
      "  (865, 1785)\t1\n",
      "  (865, 2745)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<866x2834 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9107 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bag_of_words = CountVectorizer()\n",
    "bag_of_words.fit(documents)\n",
    "word_counts = bag_of_words.transform(documents)\n",
    "\n",
    "print(word_counts)\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transform` method returns a sparse matrix. A sparse matrix is a more efficient manner of storing a matrix. If a matrix has mostly zero entries, it is better to just store the non-zero entries and their occurrence, their row and column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# get word counts\n",
    "counts_animal = bag_of_words.transform(animal_sents)\n",
    "counts_language = bag_of_words.transform(language_sents)\n",
    "\n",
    "# index for \"programming\"\n",
    "ind_programming = bag_of_words.vocabulary_['programming']\n",
    "\n",
    "# total counts across all documents\n",
    "print(counts_animal.sum(axis=0)[0, ind_programming])\n",
    "print(counts_language.sum(axis=0)[0, ind_programming])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency-inverse document frequency\n",
    "\n",
    "**CountVectorizer** creates a feature matrix of raw counts. Using raw counts has two problems, documents vary widely in length and the counts will be large for common words such as \"the\" and \"is\". We need to use a weighting scheme that considers the aforementioned attributes. The term frequency-inverse document frequency, **tf-idf** for short, is a popular weighting scheme to improve the simple count based data from the bag of words model. It is the product of two values, the term frequency and the inverse document frequency. There are several variants but the most popular is defined below.\n",
    "\n",
    "* **Term Frequency:**\n",
    "$$ \\mathrm{tf}(t, d) = \\frac{\\mathrm{counts}(t, d)}{\\sqrt{\\sum_{t \\in d} \\mathrm{counts}(t, d)^2}}, $$\n",
    "    where $\\mathrm{counts}(t, d)$ is the raw count of term $t$ in document $d$ and $t \\in d$ are the terms in document $d$. The normalization results in a vector of unit length.\n",
    "\n",
    "* **Inverse Document Frequency:**\n",
    "$$ \\mathrm{idf}(t, D) = \\ln\\left(\\frac{\\text{number of documents in corpus } D}{1 + \\text{number of documents with term } t}\\right). $$\n",
    "    Every counted term $t$ in the corpus will have its own idf weight. The $1+$ in the denominator is to ensure no division by zero if a term does not appear in the corpus. The idf weight is simply the log of the inverse of a term's document frequency.\n",
    "    \n",
    "With both $\\mathrm{tf}(t, d)$ and $\\mathrm{idf}(t, D)$ calculated, the tf-idf weight is\n",
    "\n",
    "$$ \\mathrm{tfidf}(t, d, D) = \\mathrm{tf}(t, d) \\mathrm{idf}(t, D).$$\n",
    "\n",
    "With the idf weighting, words that are very common throughout the documents get weighted down. The reverse is true; the count of rare words get weighted up. With the tf-idf weighting scheme, a machine learning model will have an easier time to learn patterns to properly predict labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2596)\t0.13953740627450906\n",
      "  (0, 2564)\t0.12010123805255166\n",
      "  (0, 2396)\t0.29868630762321047\n",
      "  (0, 2384)\t0.34820978703735816\n",
      "  (0, 2383)\t0.3608325622415421\n",
      "  (0, 2163)\t0.3291762394384941\n",
      "  (0, 2161)\t0.23654005300487888\n",
      "  (0, 2034)\t0.25611491396866004\n",
      "  (0, 1705)\t0.3216226411298091\n",
      "  (0, 1547)\t0.34820978703735816\n",
      "  (0, 1363)\t0.15772873545610774\n",
      "  (0, 282)\t0.3608325622415421\n",
      "  (0, 230)\t0.12557303557114396\n",
      "  (1, 2799)\t0.2590477608040859\n",
      "  (1, 2776)\t0.2590477608040859\n",
      "  (1, 2564)\t0.18415095607188434\n",
      "  (1, 2358)\t0.18511665219431173\n",
      "  (1, 2090)\t0.2669546385877343\n",
      "  (1, 1793)\t0.16797282373031253\n",
      "  (1, 1780)\t0.11301567849150926\n",
      "  (1, 1516)\t0.2669546385877343\n",
      "  (1, 1497)\t0.28910800238573126\n",
      "  (1, 1496)\t0.22898751922244057\n",
      "  (1, 1462)\t0.2669546385877343\n",
      "  (1, 1382)\t0.28910800238573126\n",
      "  :\t:\n",
      "  (853, 797)\t0.765489540789116\n",
      "  (854, 258)\t1.0\n",
      "  (855, 1364)\t0.4492928625542091\n",
      "  (855, 138)\t0.4492928625542091\n",
      "  (855, 96)\t0.5460182448030878\n",
      "  (855, 68)\t0.5460182448030878\n",
      "  (857, 2495)\t0.6408756444550555\n",
      "  (857, 1573)\t0.5578386820383123\n",
      "  (857, 52)\t0.5273465778461787\n",
      "  (858, 2034)\t0.20928620073594587\n",
      "  (858, 1990)\t0.41317708147185045\n",
      "  (858, 1268)\t0.23038867972627736\n",
      "  (858, 846)\t0.5522288168161412\n",
      "  (858, 75)\t0.6537953622195135\n",
      "  (859, 2757)\t0.5773502691896258\n",
      "  (859, 1984)\t0.5773502691896258\n",
      "  (859, 169)\t0.5773502691896258\n",
      "  (860, 1364)\t0.4492928625542091\n",
      "  (860, 138)\t0.4492928625542091\n",
      "  (860, 123)\t0.5460182448030878\n",
      "  (860, 81)\t0.5460182448030878\n",
      "  (863, 1493)\t0.7195810784006282\n",
      "  (863, 975)\t0.6944084328461088\n",
      "  (865, 2745)\t0.7425605561076563\n",
      "  (865, 1785)\t0.6697789340618949\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_weights = tfidf.fit_transform(word_counts)\n",
    "print(tfidf_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We no longer have raw counts in our feature matrix. Let's use the `idf_` attribute of the fitted tf-idf transformer to inspect the top idf weights and their corresponding terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.071891796220597 zope\n",
      "7.071891796220597 leone\n",
      "7.071891796220597 labs\n",
      "7.071891796220597 lacey\n",
      "7.071891796220597 ladd\n",
      "7.071891796220597 lakes\n",
      "7.071891796220597 land\n",
      "7.071891796220597 boo\n",
      "7.071891796220597 bombana\n",
      "7.071891796220597 lantz\n",
      "7.071891796220597 laos\n",
      "7.071891796220597 laptop\n",
      "7.071891796220597 bohol\n",
      "7.071891796220597 board\n",
      "7.071891796220597 blue\n",
      "7.071891796220597 last\n",
      "7.071891796220597 blotches\n",
      "7.071891796220597 blender\n",
      "7.071891796220597 latin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saurabhbudholiya/miniforge3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "top_idf_indices = tfidf.idf_.argsort()[:-20:-1]\n",
    "ind_to_word = bag_of_words.get_feature_names()\n",
    "\n",
    "for ind in top_idf_indices:\n",
    "    print(tfidf.idf_[ind], ind_to_word[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Preprocessing:-\n",
    "\n",
    "So far, we have discussed how using tf-idf rather than raw counts will improve the performance of our machine learning model. There are several other approaches that can boost performance; we will discuss techniques that improve the signal in our data set. Note, the following techniques may marginally increase model performance. It may be best to create a baseline model and measure the increased performance with the new model additions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "\n",
    "Words such as \"the\", \"a\", and \"or\" are so common throughout our corpus that they do not contribute any signal to our data set. Further, omitting these words will reduce our already high dimensional data set. It is best to not have these words as features and not be counted in the analysis. The set of words that will not factor into our analysis are called **stop words**.\n",
    "\n",
    "spaCy provides a set of around 300 commonly used English words. When using stop words, it is best to examine the entries in case there are certain words you want to be included or not included. Since the words are provided as a Python set, we can use methods available to set objects to modify entries of the set object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'set'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'could',\n",
       " 'did',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'indeed',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'made',\n",
       " 'make',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " \"n't\",\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'n‘t',\n",
       " 'n’t',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'python',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 're',\n",
       " 'really',\n",
       " 'regarding',\n",
       " 'same',\n",
       " 'say',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'under',\n",
       " 'unless',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'used',\n",
       " 'using',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " '‘d',\n",
       " '‘ll',\n",
       " '‘m',\n",
       " '‘re',\n",
       " '‘s',\n",
       " '‘ve',\n",
       " '’d',\n",
       " '’ll',\n",
       " '’m',\n",
       " '’re',\n",
       " '’s',\n",
       " '’ve'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import STOP_WORDS\n",
    "\n",
    "print(type(STOP_WORDS))\n",
    "STOP_WORDS_python = STOP_WORDS.union({\"python\"})\n",
    "STOP_WORDS_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmatization\n",
    "\n",
    "In our current analysis, words like \"python\" and \"pythons\" will be counted as separate words. We understand that they represent the same concept and want them to be treated as the same word. The same applies to other words like \"run\", \"runs\", \"ran\", and \"running\", they all represent the same meaning. **Stemming** is the process of reducing a word to its stem. Note, the stemming process is not 100% effective and sometimes the resulting stem is not an actual word. For example, the popular Porter stemming algorithm applied to \"argues\" and \"arguing\" returns **argu**.\n",
    "\n",
    "**Lemmatization** is the process of reducing a word to its lemma, or the dictionary form of the word. It is a more sophisticated process than stemming as it considers context and part of speech. Further, the resulting lemma is an actual word. spaCy does not have a stemming algorithm but does offer lemmatization. Each word analyzed by spaCy has the attribute **lemma_** which returns the lemma of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'run', 'run', 'run']\n",
      "['buy', 'buy', 'buying', 'buy']\n",
      "['see', 'saw', 'see', 'see']\n"
     ]
    }
   ],
   "source": [
    "print([word.lemma_ for word in nlp('run runs ran running')])\n",
    "print([word.lemma_ for word in nlp('buy buys buying bought')])\n",
    "print([word.lemma_ for word in nlp('see saw seen seeing')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saurabhbudholiya/miniforge3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '\\n\\n', '\\n\\n\\n', ' ', '\"', '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', ':', ';', '=', 'add', 'allow', 'ball', 'block', 'breed', 'c', 'captivity', 'class', 'code', 'common', 'compile', 'cpython', 'describe', 'design', 'development', 'division', 'e.g.', 'eat', 'egg', 'example', 'expression', 'feature', 'female', 'find', 'ft', 'function', 'human', 'implementation', 'include', 'integer', 'island', 'java', 'kill', 'language', 'large', 'later', 'length', 'library', 'like', 'list', 'long', 'm.', 'male', 'measure', 'method', 'module', 'new', 'number', 'object', 'old', 'operator', 'oz', 'pattern', 'prey', 'program', 'programming', 'propose', 'provide', 'r.', 'range', 'reference', 'release', 'report', 'reticulate', 'reticulated', 'small', 'snake', 'specie', 'standard', 'statement', 'string', 'support', 'syntax', 'time', 'type', 'value', 'variable', 'version', 'write', 'year']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def lemmatizer(text):\n",
    "    return [word.lemma_ for word in nlp(text)]\n",
    "\n",
    "# we need to generate the lemmas of the stop words\n",
    "stop_words_str = \" \".join(STOP_WORDS) # nlp function needs a string\n",
    "stop_words_lemma = set(word.lemma_ for word in nlp(stop_words_str))\n",
    "\n",
    "tfidf_lemma = TfidfVectorizer(max_features=100, \n",
    "                              stop_words=stop_words_lemma.union({\"python\"}),\n",
    "                              tokenizer=lemmatizer)\n",
    "\n",
    "tfidf_lemma.fit(documents)\n",
    "print(tfidf_lemma.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and n-grams\n",
    "\n",
    "Tokenization refers to dividing up a document into pieces to be counted. In our analysis so far, we are only counting words. However, it may be useful to count a sequence of words such as \"natural environment\" and \"virtual environment\". Counting these **bigrams** for our word usage analyzer may boost performance. More generally, an n-gram refers to the n sequence of words. In `scikit-learn`, n-grams can be included by setting `ngram_range=(min_n, max_n)` for the vectorizer, where `min_n` and `max_n` are the lower and upper bound of the range of n-grams to include. For example, `ngram_range=(1, 2)` will include words and bigrams while `ngram_range=(2, 2)` will only count bigrams. Let's see what are the most frequent bigrams in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saurabhbudholiya/miniforge3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['23 ft',\n",
       " 'ball pythons',\n",
       " 'design philosophy',\n",
       " 'floating point',\n",
       " 'ft 10',\n",
       " 'ft length',\n",
       " 'isbn 978',\n",
       " 'lb oz',\n",
       " 'new features',\n",
       " 'object oriented',\n",
       " 'oriented programming',\n",
       " 'programming language',\n",
       " 'programming languages',\n",
       " 'reference implementation',\n",
       " 'reticulated pythons',\n",
       " 'scripting language',\n",
       " 'spam eggs',\n",
       " 'standard library',\n",
       " 'van rossum',\n",
       " 'year old']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_counter=CountVectorizer(max_features=20, ngram_range=(2,2), stop_words=STOP_WORDS.union({\"python\"}))\n",
    "bigram_counter.fit(documents)\n",
    "\n",
    "bigram_counter.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document similarity\n",
    "\n",
    "After we have transformed our corpus into a matrix, we can interpret our data set as representing a set of vectors in a $p$-dimensional space, where each document is its own vector. One common analysis is to find similar documents. The cosine similarity is a metric that measure how well aligned in space are two vectors, equal to the cosine of the angle in between the two vectors. If the vectors are perfectly aligned, they point in the same direction, the angle they form is 0 and the similarity score is 1. If the vectors are orthogonal, forming an angle of 90 degrees, the similarity metric is 0. Mathematically, the cosine similarity metric is equal to the dot product of two vectors, normalized,\n",
    "\n",
    "$$ \\frac{v_1 \\cdot v_2}{\\|v_1 \\|\\|v_2 \\|}, $$\n",
    "\n",
    "where $v_1$ and $v_2$ are two document vectors and $\\| v_1 \\|$ and $\\| v_2 \\|$ are their lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word usage classifier\n",
    "\n",
    "Let's build a word usage classifier with all the techniques we have seen. The model will include:\n",
    "\n",
    "* tf-idf weighting\n",
    "* stop words\n",
    "* words and bigrams\n",
    "* lemmatization\n",
    "\n",
    "Applying the above techniques should result in a data set with enough signal that a machine learning model can learn from. For this exercise, we will use the naive Bayes model; a probabilistic model that calculates conditional probabilities using Bayes theorem. The term naive is applied because it assumes the features are conditionally independent from each other. You can think of a naive Bayes classifier working by determining what class should a document be assigned based upon the frequencies of words in the different classes in the training set. Naive Bayes is often used as benchmark model for NLP as it is quick to train. More about the model in general can be found [here](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) and details of the `scikit-learn` implementation is found [here](https://scikit-learn.org/stable/modules/naive_bayes.html). After training our model, we will see how well it performs for a chosen set of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8995381062355658\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# create data set and labels\n",
    "documents = animal_sents + language_sents\n",
    "labels = [\"animal\"]*len(animal_sents) + [\"language\"]*len(language_sents)\n",
    "\n",
    "# lemma of stop words\n",
    "stop_words_str = \" \".join(STOP_WORDS)\n",
    "stop_words_lemma = set(word.lemma_ for word in nlp(stop_words_str))\n",
    "\n",
    "# create and train pipeline\n",
    "tfidf = TfidfVectorizer(stop_words=stop_words_lemma, tokenizer=lemmatizer, ngram_range=(1, 2))\n",
    "pipe = Pipeline([('vectorizer', tfidf), ('classifier', MultinomialNB())])\n",
    "pipe.fit(documents, labels)\n",
    "\n",
    "print(\"Training accuracy: {}\".format(pipe.score(documents, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Python program is only 100 bytes long. --> language at 69.1922%\n",
      "A python's bite is not venomous but still hurts. --> animal at 51.0419%\n",
      "I can't find the error in the python code. --> language at 80.4892%\n",
      "Where is my pet python; I can't find her! --> language at 55.9227%\n",
      "I use for and while loops when writing Python. --> language at 85.1932%\n",
      "The python will loop and wrap itself onto me. --> language at 69.4983%\n",
      "I use snake case for naming my variables. --> language at 59.8769%\n",
      "My python has grown to over 10 ft long! --> animal at 60.9395%\n",
      "I use virtual environments to manage package versions. --> language at 80.3619%\n",
      "Pythons are the largest snakes in the environment. --> animal at 63.807%\n"
     ]
    }
   ],
   "source": [
    "test_docs = [\"My Python program is only 100 bytes long.\",\n",
    "             \"A python's bite is not venomous but still hurts.\",\n",
    "             \"I can't find the error in the python code.\",\n",
    "             \"Where is my pet python; I can't find her!\",\n",
    "             \"I use for and while loops when writing Python.\",\n",
    "             \"The python will loop and wrap itself onto me.\",\n",
    "             \"I use snake case for naming my variables.\",\n",
    "             \"My python has grown to over 10 ft long!\",\n",
    "             \"I use virtual environments to manage package versions.\",\n",
    "             \"Pythons are the largest snakes in the environment.\"]\n",
    "\n",
    "class_labels = [\"animal\", \"language\"]\n",
    "y_proba = pipe.predict_proba(test_docs)\n",
    "predicted_indices = (y_proba[:, 1] > 0.5).astype(int)\n",
    "\n",
    "for i, index in enumerate(predicted_indices):\n",
    "    print(test_docs[i], \"--> {} at {:g}%\".format(class_labels[index], 100*y_proba[i, index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
